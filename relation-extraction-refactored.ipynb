{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-21T00:15:17.656281Z",
     "start_time": "2024-11-21T00:15:15.147756Z"
    }
   },
   "source": "%pip install -q langchain requests ollama langchain-ollama langchain-community aim streamlit",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Text Chunking\n",
    "\n",
    "For the sake of model max token limits, we need to process the text in chunks."
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-04T18:29:33.457075Z",
     "start_time": "2024-12-04T18:29:33.448468Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Text chunking\n",
    "\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.document_loaders import TextLoader\n",
    "\n",
    "loader = TextLoader(\"./data/a-xmas-carol-body.txt\")\n",
    "documents = loader.load()\n",
    "\n",
    "# Initialize the splitter\n",
    "splitter = CharacterTextSplitter(\n",
    "    separator =\"\\n\\n\",\n",
    "    chunk_size=1024,\n",
    "    chunk_overlap = 256,\n",
    ")\n",
    "\n",
    "chunks = splitter.split_documents(documents)\n",
    "\n",
    "print(f'{len(chunks)} chunks created')"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 1603, which is longer than the specified 1024\n",
      "Created a chunk of size 1127, which is longer than the specified 1024\n",
      "Created a chunk of size 1143, which is longer than the specified 1024\n",
      "Created a chunk of size 1668, which is longer than the specified 1024\n",
      "Created a chunk of size 1754, which is longer than the specified 1024\n",
      "Created a chunk of size 1224, which is longer than the specified 1024\n",
      "Created a chunk of size 1207, which is longer than the specified 1024\n",
      "Created a chunk of size 1158, which is longer than the specified 1024\n",
      "Created a chunk of size 1763, which is longer than the specified 1024\n",
      "Created a chunk of size 1467, which is longer than the specified 1024\n",
      "Created a chunk of size 1070, which is longer than the specified 1024\n",
      "Created a chunk of size 1147, which is longer than the specified 1024\n",
      "Created a chunk of size 1260, which is longer than the specified 1024\n",
      "Created a chunk of size 1747, which is longer than the specified 1024\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "203 chunks created\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Relation Extraction"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "First, let's ask the LLM to provide us with a list of possible node / relation types."
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-04T16:43:36.676086Z",
     "start_time": "2024-12-04T16:43:23.159694Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "node_types_json_schema = {\n",
    "    \"title\": \"entity types\",\n",
    "    \"description\": \"a list of entity types\",\n",
    "    \"type\": \"object\",\n",
    "    \"properties\": {\n",
    "        \"entity types\": {\n",
    "            \"type\": \"string\",\n",
    "            \"description\": \"A JSON-formatted list of strings\"\n",
    "        }\n",
    "    },\n",
    "    \"required\": [\"entity types\"]\n",
    "}\n",
    "\n",
    "llama3_1_node_type = ChatOllama(\n",
    "    model=\"llama3.1:8b\",\n",
    "    temperature=0.8,\n",
    "    top_k=60,\n",
    "    top_p=0.9,\n",
    "    num_predict=2048,\n",
    "    base_url=\"http://127.0.0.1:11434\",\n",
    ")\n",
    "\n",
    "llama3_1_node_type_structured = llama3_1_node_type.with_structured_output(node_types_json_schema)\n",
    "\n",
    "entity_type_prompt_template = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"You are to assist the user to construct a knowledge graph for a novel. \"),\n",
    "        (\"human\", \"\"\"\n",
    "The user is working on extracting entities from a book to build a knowledge graph. You goal is to provide a list of possible entity types that are commonly present in knowledge graphs for novels. Please provide a list of such entity types.\n",
    "\"\"\"),\n",
    "    ])\n",
    "\n",
    "entity_type_chain = entity_type_prompt_template | llama3_1_node_type_structured\n",
    "\n",
    "result = entity_type_chain.invoke(input={})\n",
    "\n",
    "import json\n",
    "node_types_list = json.loads(result[\"entity types\"])\n",
    "print(node_types_list)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Person', 'Organization', 'Location', 'Event', 'Date']\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Relation Extraction by Chunk\n",
    "\n",
    "Let's define our prompt and model for the extraction task."
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-04T18:29:47.772215Z",
     "start_time": "2024-12-04T18:29:47.740219Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import yaml\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "from utils.extract_util_v2 import convert_to_graph_documents, parse_response\n",
    "\n",
    "# read prompt files from yaml\n",
    "with open('./prompts/entity.yaml', 'r') as file:\n",
    "    entity_yaml = yaml.safe_load(file)\n",
    "    \n",
    "from typing import Any, Dict, List, Optional, Sequence, Tuple, Type, Union, cast\n",
    "\n",
    "from utils.extract_util_v2 import create_schema, parse_response\n",
    "\n",
    "system_prompt = (\n",
    "    \"# Knowledge Graph Instructions for GPT-4\\n\"\n",
    "    \"## 1. Overview\\n\"\n",
    "    \"You are a top-tier algorithm designed for extracting information in structured \"\n",
    "    \"formats to build a knowledge graph.\\n\"\n",
    "    \"Try to capture as much information from the text as possible without \"\n",
    "    \"sacrificing accuracy. Do not add any information that is not explicitly \"\n",
    "    \"mentioned in the text.\\n\"\n",
    "    \"- **Nodes** represent entities and concepts.\\n\"\n",
    "    \"- The aim is to achieve simplicity and clarity in the knowledge graph, making it\\n\"\n",
    "    \"accessible for a vast audience.\\n\"\n",
    "    \"## 2. Labeling Nodes\\n\"\n",
    "    \"- **Consistency**: Ensure you use available types for node labels.\\n\"\n",
    "    \"Ensure you use basic or elementary types for node labels.\\n\"\n",
    "    \"- For example, when you identify an entity representing a person, \"\n",
    "    \"always label it as **'person'**. Avoid using more specific terms \"\n",
    "    \"like 'mathematician' or 'scientist'.\"\n",
    "    \"- **Node IDs**: Never utilize integers as node IDs. Node IDs should be \"\n",
    "    \"names or human-readable identifiers found in the text.\\n\"\n",
    "    \"- **Relationships** represent connections between entities or concepts.\\n\"\n",
    "    \"Ensure consistency and generality in relationship types when constructing \"\n",
    "    \"knowledge graphs. Instead of using specific and momentary types \"\n",
    "    \"such as 'BECAME_PROFESSOR', use more general and timeless relationship types \"\n",
    "    \"like 'PROFESSOR'. Make sure to use general and timeless relationship types!\\n\"\n",
    "    \"## 3. Coreference Resolution\\n\"\n",
    "    \"- **Maintain Entity Consistency**: When extracting entities, it's vital to \"\n",
    "    \"ensure consistency.\\n\"\n",
    "    'If an entity, such as \"John Doe\", is mentioned multiple times in the text '\n",
    "    'but is referred to by different names or pronouns (e.g., \"Joe\", \"he\"),'\n",
    "    \"always use the most complete identifier for that entity throughout the \"\n",
    "    'knowledge graph. In this example, use \"John Doe\" as the entity ID.\\n'\n",
    "    \"Remember, the knowledge graph should be coherent and easily understandable, \"\n",
    "    \"so maintaining consistency in entity references is crucial.\\n\"\n",
    "    \"## 4. Strict Compliance\\n\"\n",
    "    \"Adhere to the rules strictly. Non-compliance will result in termination.\"\n",
    ")\n",
    "\n",
    "default_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            system_prompt,\n",
    "        ),\n",
    "        (\n",
    "            \"human\",\n",
    "            (\n",
    "                \"Tips: \"\n",
    "                \"1. Make sure to answer in the correct format and do \"\n",
    "                \"not include any explanations. \"\n",
    "                \"2. Do not include newline or special symbols such as \\\\\\\\ in your response that might cause python parsing errors. \"\n",
    "                \"3. Stick to the given structured output schema. `Node` has `id` and `type` fields, and `Relationship` has `source`, `target`, and `type` fields. \"\n",
    "                \"4. Extract information from the \"\n",
    "                \"following input: {text}\"\n",
    "            ),\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "llama3_1 = ChatOllama(\n",
    "    model=\"llama3.1:8b\",\n",
    "    temperature=0.2,\n",
    "    top_k=10,\n",
    "    top_p=0.6,\n",
    "    num_predict=2048,\n",
    "    base_url=\"http://127.0.0.1:11434\",\n",
    ")\n",
    "\n",
    "llama3_1_node_structured = llama3_1.with_structured_output(create_schema(), include_raw=True)  \n",
    "\n",
    "relation_extraction_chain = default_prompt | llama3_1_node_structured"
   ],
   "outputs": [],
   "execution_count": 20
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "A quick test to see if the model can extract entities from the first chunk."
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-04T16:43:42.705224Z",
     "start_time": "2024-12-04T16:43:36.725962Z"
    }
   },
   "cell_type": "code",
   "source": [
    "result = relation_extraction_chain.invoke({\n",
    "    \"text\": chunks[5].page_content,\n",
    "})\n",
    "\n",
    "parse_response(result)"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([Node(id='human sympathy', type='concept', properties={}),\n",
       "  Node(id='Scrooge', type='person', properties={})],\n",
       " [Relationship(source=Node(id='Scrooge', type='person', properties={}), target=Node(id='human sympathy', type='concept', properties={}), type='DISTANCE', properties={})])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Now, let's extract KGs from all the chunks."
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-04T18:51:35.357193Z",
     "start_time": "2024-12-04T18:29:54.687615Z"
    }
   },
   "cell_type": "code",
   "source": "graph_documents = convert_to_graph_documents(chunks, relation_extraction_chain)",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing the documents...:   0%|          | 0/203 [00:00<?, ?it/s, Current chunk=CONTENTS\n",
      "\n",
      "Processing the documents...:   0%|          | 1/203 [00:33<1:54:06, 33.89s/it, Current chunk=CONTENTS\n",
      "\n",
      "Processing the documents...:  24%|██▍       | 49/203 [05:21<12:59,  5.06s/it, Current chunk=STAVE TWOh..., Iteration=49]\n",
      "Processing the documents...:  25%|██▍       | 50/203 [05:28<14:07,  5.54s/it, Current chunk=STAVE TWO\n",
      "Processing the documents...:  40%|████      | 82/203 [09:22<14:10,  7.03s/it, Current chunk=He seemed ..., Iteration=84]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw response message parsing error: 'tool_calls'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing the documents...: 100%|█████████▉| 202/203 [21:40<00:06,  6.44s/it, Current chunk=He had no ..., Iteration=203]\n"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-04T18:57:47.124967Z",
     "start_time": "2024-12-04T18:57:47.114600Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# cache our results\n",
    "import pickle\n",
    "\n",
    "with open('./output/graph_documents/graph_documents_refactored.pkl', 'wb') as f:\n",
    "    pickle.dump(graph_documents, f)"
   ],
   "outputs": [],
   "execution_count": 22
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Entity Canonicalization / Disambiguation\n",
    "\n",
    "Associate mentions of entities with an appropriate disambiguated KB identifier (id).\n",
    "\n",
    "Combine relationships from different chunks and resolve conflicts across chunks."
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-04T23:05:37.483858Z",
     "start_time": "2024-12-04T23:05:37.478544Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from utils.extract_util_v2 import merge_graphs\n",
    "\n",
    "merged_graph_document = merge_graphs(graph_documents=graph_documents,\n",
    "             source_document=documents[0])"
   ],
   "outputs": [],
   "execution_count": 68
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-05T14:22:52.593688Z",
     "start_time": "2024-12-05T14:22:52.587172Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pickle\n",
    "with open('./output/graph_documents/merged_graph_document.pkl', 'wb') as f:\n",
    "    pickle.dump(merged_graph_document, f)"
   ],
   "outputs": [],
   "execution_count": 94
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-05T16:58:00.626763Z",
     "start_time": "2024-12-05T16:58:00.613192Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# load the cached merged graph documents, just in case\n",
    "import pickle\n",
    "with open('./output/graph_documents/merged_graph_document.pkl', 'rb') as f:\n",
    "    merged_graph_document = pickle.load(f)"
   ],
   "outputs": [],
   "execution_count": 100
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Langchain-Neo4j Exporter"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-05T16:15:02.357564Z",
     "start_time": "2024-12-05T16:15:02.248510Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_neo4j import Neo4jGraph\n",
    "import dotenv, os\n",
    "\n",
    "dotenv.load_dotenv()\n",
    "\n",
    "URI = os.environ['NEO4J_URI']\n",
    "USER = os.environ['NEO4J_USERNAME']\n",
    "PASSWORD = os.environ['NEO4J_PASSWORD']\n",
    "\n",
    "# initialize a neo4j-langchain graph\n",
    "graph = Neo4jGraph(refresh_schema=False)\n",
    "\n",
    "from utils.neo4j_util import Neo4jGraphImporter\n",
    "\n",
    "neo4j_importer = Neo4jGraphImporter(\n",
    "        uri=URI, \n",
    "        user=USER, \n",
    "        password='password',\n",
    "    )\n",
    "\n",
    "# clean the database\n",
    "neo4j_importer.clear_database()  # Optional: Clear the database\n",
    "neo4j_importer.drop_all_constraints() # Optional: Drop all constraints"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Database cleared.\n",
      "All constraints dropped.\n"
     ]
    }
   ],
   "execution_count": 95
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-05T16:36:59.889197Z",
     "start_time": "2024-12-05T16:36:51.998526Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# add the merged graph document to the neo4j graph\n",
    "graph.add_graph_documents([merged_graph_document])"
   ],
   "outputs": [],
   "execution_count": 99
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": ""
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
