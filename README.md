# kg-construction

This project aims at constructing a knowledge graph for Charles Dickens' novel _A Christmas Carol_ using local Large Language Models (LLMs).

In essence, it can be also regarded as a Document-level Relation Triplet Extraction (DocRTE) problem, using LLMs with zero-shot / few-shot capabilities, for the given text is unstructured and no annotated data is available.

Tech Stack: langchain, networkx, neo4j

LLM used in the project: LLama 3.1 8B

## Steps to Reproduce

### Prerequisites

Install Ollama locally, for mac users:

```bash
brew install ollama
```

After installation, pull [Llama 3.1 8B model](https://ollama.com/library/llama3.1):

```bash
ollama run llama3.1:8b
```

Make sure your ollama is serving the model at `http://localhost:11434`.

Create a virtual environment and install the required packages. For example, using conda:

```bash
conda create -n kg-construction python=3.11
conda activate kg-construction
pip install -r requirements.txt
```

### Relation Extraction

Refer to relation-extraction.ipynb for a step by step guide.

Instead, you can run the .py version script directly.

```bash
python relation_extraction.py
```

After running the script, you will get a list of relation triplets extracted from the text at `output/relations/chunk-level-relations.pickle`,  together with the corresponding entity types at `output/relations/relation-entity-type.pickle`.

> [!NOTE]  
> No random seed is set in the script, so the results may vary each time you run it.

### Knowledge Graph Construction and Visualization

After extracting the relation triplets, you can construct the knowledge graph and visualize it.

The notebook for constructing the knowledge graph is `./kg-construction.ipynb` and offers a step-by-step guide.



#### Streamlit App

Alternatively, you can start a simple [streamlit](https://streamlit.io) app to visualize the knowledge graph, 

```bash
streamlit run showcase.py
```

![image](https://github.com/user-attachments/assets/a651f53e-be55-4ef3-9308-74198d446c9e)


#### Neo4j 

The notebook also provides a guide on how to export the networkx graph to a neo4j database. 

![image](https://github.com/user-attachments/assets/d75bac19-d456-4fd8-ad5a-160518a7f92a)


You can choose between a local neo4j instance or a free-tier AuraDB instance.

To set a local neo4j instance, you can follow the instructions on their [official website](https://neo4j.com/download/).

For mac users, you can install it via brew:

```bash
brew install neo4j
```

and start the neo4j console:

```bash
neo4j console
```

and fill in your credentials in the notebook to connect to the local neo4j instance.

To set up a free-tier AuraDB instance, you can follow the instructions on their [official website](https://neo4j.com/aura/),
and modify the driver credentials in the notebook accordingly.

#### GraphML

The graphML file is also available at `output/KGs/kg.graphml`.


## Methodology

The key to constructing a knowledge graph from unstructured text is to extract entities and relations from the text, 
in which relations are of greater importance in our case, given my experience that it is hard to guide local LLMs with fewer
parameters to produce matching relation results with the given entity list. 

Due to the context window limitation of the LLM, we have to do text chunking and then relation extraction, 
but the chunks themselves are not representative of the entire document, 
so we need to maintain a global context across all chunks, which is also short enough to be fed together into the LLM.

A lot of such context mechanism could be implemented, but for the sake of simplicity,
I only prepend a global list of most important entities as the context, which is also generated by the LLM, gathered from
local entity extraction results.

Let me explain the pipeline next.

### Entity Extraction

The first step is to generate the global entity list.

**Entity type Generation**: First, let the LLM think for us about what kinds of entities could possibly be extracted. 
This will serve as the blueprint. 

Provide a prompt for the LLM to generate possible entity types that could fit in the context of the document. 
The prompt design is zero shot, providing no examples. Model parameters are set to be more creative.


**Chunk-level entity extraction**: Chunk the text, and prompt the LLM to grab entities list on each chunk, which serves
as our local entity list. The prompt design is few shot, providing some examples of correct outcomes. 
Model parameters are set to be more precise.

My initial idea is to gather the local lists and merge them into a global list, but it is not that easy, due to the
fact that it is still too lengthy to be fed in. So my next step is,

**Stave-level entity extraction**: Let the LLM figure out what is the most important entities in each stave, and then aggregate them into

**Global-level entity list**. These are all done by prompt engineering, which is time consuming and error prone.


### Relation Extraction

The relation extraction process follows a two step paradigm.

First, together with the global entity list and local chunks of texts, we prompt the LLM to extract relation triplets on each chunk.

Also, we asked the LLM to think about the possible relation types, which is then used for the relation extraction.

Then, for each relation triplet, let the LLMs resolve the entity types from the assigned list of possible entity types.

I choose not to do an aggregation on chunk-level relations, being that the hallucination problem is severe.

The final set of relation triplets is then used to construct the knowledge graph.

### Implementation

The whole pipeline is implemented in langchain, following the thought-of-chain of the LLM to derive the final set of extracted relation triplets. It could also be integrated into a super large chain.


# Quality Evaluation & Challenges

This section will focus on the intrinsic features of the quality of the graph. It is helpful to consider the most used dimensions of data quality: accuracy, consistency, completeness and timeliness.

**Accuracy:**

The relation triplets to form the KG are extracted on a chunk level basis, though it is compromised by the general hallucination problem.

- For fear of hallucination, in the relation extraction process, I opted out of using the LLM to summarize and resolve repetitions,
  which could lead to unexpected, plausible yet incorrect factual information in the results.
  Multi-level extraction are prone to such problems. 
  For complicated chain-of-thought pipelines involving aggregation tasks, it is hard to measure the imprecision of the model outcomes.
  
- As a result, I noticed many incorrect entities in the final global entity list which is supposed to be a global context of the document.

**Consistency**

- Redundancy / Coreference Resolution: If we follow a relation-centric approach, using the extracted relation to fill in the knowledge map, chances are some of the entities might not have a consistent reference. This might result in multiple different nodes in the graph actually refering to the same entity. A key step for a good knowledge graph is to resolve such coreferences, but it poses challanges for LLMs (at least for small models) to identify and merge mentions of the same entity across different chunks. It is also difficult to use LLM to match entities extracted from entity extractions process and those extracted from relation extraction process.
- Trivial Information: A quick glance at the extracted relations shows that some of the relations are taken down serving a more semantic purpose than practicality. I made several attempts on the prompts, but the model still had trouble distinguishing the important relations from the trivial ones.

**Completeness**

Model's output token size `num_predict` sets a cap on the number of relations that can be extracted from a single chunk.
Smaller `num_predict` restricts the model to predicting only a few relations, while most local models often have a cap of output size of 4096. This does not affect smaller text chunks or aggregations on relatively short chapters, but for aggregation of relation extraction on long, or even entire document, this might hit a bottleneck.

**Timeliness**

The pipeline is build by chaining the LLMs, which takes several turns to run (approx. 25 min) locally on my macbook pro.
Although we cannot prevent the LLM to malfunction, it is however possible to do necessary error handling work so that 
the pipeline can be containerized and fit into a CI/CD scheme, and triggered when the upstream document is updated.


Apart from the data quality point of view, there is also the aspect of the robustness of the pipeline. The LLM output
is capricious, in the sense of:

- Formats: Even though a output format template is provided, the LLM's output does not always strictly comply with the given instruction. Anomalous valuesÂ and issing records occur from time to time, which will compromise the robustness of text-processing pipeline.  For example, in the last step of the project, the LLM was supposed to resolve the entity types given a list of relations. However, in the first few attempts the model's answers did not match the length of the input, so I had to switch to another approach.
- Glitches: If the output requires complicated formatting, such as single quotes, double quotes, nested lists
- All of these means our pipelines require more prompt engineering work as well as error handling.



# Future Improvement

- Prompt Engineering: There are several aspects to consider an improvement of prompts
  - Adaptation to chunks: for the context message provide more chunk-specific information (ids, locations in the document, summary, etc.)
  - An LSTM-like prompt structure, in which the context of previous chunk is also used. Maintaining a dynamic entity list.
- More experiments
  - Running LLMs locally is time consuming, so the parameters are not optimized in this project,
  but a parameter search might improve the outcomes. Also the text chunking strategy could be optimized 
  in terms of the granularity of the chunks, overlapping chunks, etc.
- There are some other works on DocRE (Document-level relation extraction) using LLMs. For example, Li et al. (2023) 
proposed a Document-relation-head-facts (DRHF) paradigm that focuses on the relation to derive an appropriate set of entities (head). Then from these entities, the most relevant relation (facts) are extracted [1]. Sun et al. (2024) proposed a zero-shot DocRE model that utilizes what they call a chain-of-Retrieval paradigm to prompt the LLMs to synthesize text and learn relations from the synthesized text, enhancing the ability of generalization of the model [2].


[1] Li, J., Jia, Z., & Zheng, Z. (2023). Semi-automatic Data Enhancement for Document-Level Relation Extraction with Distant Supervision from Large Language Models (No. arXiv:2311.07314). arXiv. https://doi.org/10.48550/arXiv.2311.07314

[2] Sun, Q., Huang, K., Yang, X., Tong, R., Zhang, K., & Poria, S. (2024). Consistency Guided Knowledge Retrieval and Denoising in LLMs for Zero-shot Document-level Relation Triplet Extraction (No. arXiv:2401.13598). arXiv. https://doi.org/10.48550/arXiv.2401.13598
